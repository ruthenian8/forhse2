{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import pyLDAvis.gensim\n",
    "import string\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "morph = MorphAnalyzer()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем 10 тыс статьи с Википедии. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительные стоп-слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional = [\"c\",\"а\",\"алло\",\"без\",\"белый\",\"возле\",\"благодаря\",\"близко\",\"более\",\"больше\",\"большой\",\"будем\",\"будет\",\"будете\",\"будешь\",\"будто\",\"буду\",\"будут\",\"будь\",\"бы\",\\\n",
    " \"бывает\",\"бывь\",\"был\",\"была\",\"были\",\"было\",\"быть\",\"в\",\"важная\",\"важное\",\"важные\",\"важный\",\"вам\",\"вами\",\"вас\",\"ваш\",\"ваша\",\"ваше\",\\\n",
    " \"ваши\",\"вверх\",\"вдали\",\"вдруг\",\"ведь\",\"везде\",\"вернуться\",\"весь\",\"вечер\",\"взгляд\",\"взять\",\"вид\",\"видел\",\"видеть\",\"вместе\",\"вне\",\\\n",
    " \"вниз\",\"внизу\",\"во\",\"вода\",\"война\",\"вокруг\",\"вон\",\"вообще\",\"вопрос\",\"восемнадцатый\",\"восемнадцать\",\"восемь\",\"восьмой\",\"вперёд\",\"вот\",\\\n",
    " \"впрочем\",\"времени\",\"время\",\"все\",\"все еще\",\"всегда\",\"всего\",\"всем\",\"всеми\",\"всему\",\"всех\",\"всею\",\"всю\",\"всюду\",\"вся\",\"всё\",\\\n",
    " \"второй\",\"вы\",\"выйти\",\"г\",\"где\",\"главный\",\"глаз\",\"говорил\",\"говорит\",\"говорить\",\"год\",\"года\",\"году\",\"голова\",\"голос\",\"город\",\\\n",
    " \"да\",\"давать\",\"давно\",\"даже\",\"далекий\",\"далеко\",\"дальше\",\"даром\",\"дать\",\"два\",\"двадцатый\",\"двадцать\",\"две\",\"двенадцатый\",\\\n",
    " \"двенадцать\",\"дверь\",\"двух\",\"девятнадцатый\",\"девятнадцать\",\"девятый\",\"девять\",\"действительно\",\"дел\",\"делал\",\"делать\",\"делаю\",\\\n",
    " \"дело\",\"день\",\"деньги\",\"десятый\",\"десять\",\"для\",\"до\",\"довольно\",\"долго\",\"должен\",\"должно\",\"должный\",\"дом\",\"дорога\",\"друг\",\\\n",
    " \"другая\",\"другие\",\"других\",\"друго\",\"другое\",\"другой\",\"думать\",\"душа\",\"е\",\"его\",\"ее\",\"ей\",\"ему\",\"если\",\"есть\",\"еще\",\"ещё\",\\\n",
    " \"ею\",\"её\",\"ж\",\"ждать\",\"же\",\"жена\",\"женщина\",\"жизнь\",\"жить\",\"за\",\"занят\",\"занята\",\"занято\",\"заняты\",\"затем\",\"зато\",\"зачем\",\\\n",
    " \"здесь\",\"земля\",\"знать\",\"значит\",\"значить\",\"и\",\"иди\",\"идти\",\"из\",\"или\",\"им\",\"имеет\",\"имел\",\"именно\",\"иметь\",\"ими\",\"имя\",\\\n",
    " \"иногда\",\"их\",\"к\",\"каждая\",\"каждое\",\"каждые\",\"каждый\",\"кажется\",\"казаться\",\"как\",\"какая\",\"какой\",\"кем\",\"книга\",\"когда\",\"кого\",\\\n",
    " \"ком\",\"комната\",\"кому\",\"конец\",\"конечно\",\"которая\",\"которого\",\"которой\",\"которые\",\"который\",\"которых\",\"кроме\",\"кругом\",\"кто\",\\\n",
    " \"куда\",\"лежать\",\"лет\",\"ли\",\"лицо\",\"лишь\",\"лучше\",\"любить\",\"люди\",\"м\",\"маленький\",\"мало\",\"мать\",\"машина\",\"между\",\"меля\",\"менее\",\\\n",
    " \"меньше\",\"меня\",\"место\",\"миллионов\",\"мимо\",\"минута\",\"мир\",\"мира\",\"мне\",\"много\",\"многочисленная\",\"многочисленное\",\"многочисленные\",\\\n",
    " \"многочисленный\",\"мной\",\"мною\",\"мог\",\"могу\",\"могут\",\"мож\",\"может\",\"может быть\",\"можно\",\"можхо\",\"мои\",\"мой\",\"мор\",\"москва\",\"мочь\",\\\n",
    " \"моя\",\"моё\",\"мы\",\"на\",\"наверху\",\"над\",\"надо\",\"назад\",\"наиболее\",\"найти\",\"наконец\",\"нам\",\"нами\",\"народ\",\"нас\",\"начала\",\"начать\",\"наш\",\\\n",
    " \"наша\",\"наше\",\"наши\",\"не\",\"него\",\"недавно\",\"недалеко\",\"нее\",\"ней\",\"некоторый\",\"нельзя\",\"нем\",\"немного\",\"нему\",\"непрерывно\",\"нередко\",\\\n",
    " \"несколько\",\"нет\",\"нею\",\"неё\",\"ни\",\"нибудь\",\"ниже\",\"низко\",\"никакой\",\"никогда\",\"никто\",\"никуда\",\"ним\",\"ними\",\"них\",\"ничего\",\"ничто\",\\\n",
    " \"но\",\"новый\",\"нога\",\"ночь\",\"ну\",\"нужно\",\"нужный\",\"нх\",\"о\",\"об\",\"оба\",\"обычно\",\"один\",\"одиннадцатый\",\"одиннадцать\",\"однажды\",\"однако\",\\\n",
    " \"одного\",\"одной\",\"оказаться\",\"окно\",\"около\",\"он\",\"она\",\"они\",\"оно\",\"опять\",\"особенно\",\"остаться\",\"от\",\"ответить\",\"отец\",\"откуда\",\\\n",
    " \"отовсюду\",\"отсюда\",\"очень\",\"первый\",\"перед\",\"писать\",\"плечо\",\"по\",\"под\",\"подойди\",\"подумать\",\"пожалуйста\",\"позже\",\"пойти\",\"пока\",\\\n",
    " \"пол\",\"получить\",\"помнить\",\"понимать\",\"понять\",\"пор\",\"пора\",\"после\",\"последний\",\"посмотреть\",\"посреди\",\"потом\",\"потому\",\"почему\",\\\n",
    " \"почти\",\"правда\",\"прекрасно\",\"при\",\"про\",\"просто\",\"против\",\"процентов\",\"путь\",\"пятнадцатый\",\"пятнадцать\",\"пятый\",\"пять\",\"работа\",\\\n",
    " \"работать\",\"раз\",\"разве\",\"рано\",\"раньше\",\"ребенок\",\"решить\",\"россия\",\"рука\",\"русский\",\"ряд\",\"рядом\",\"с\",\"с кем\",\"сам\",\"сама\",\"сами\",\\\n",
    " \"самим\",\"самими\",\"самих\",\"само\",\"самого\",\"самой\",\"самом\",\"самому\",\"саму\",\"самый\",\"свет\",\"свое\",\"своего\",\"своей\",\"свои\",\"своих\",\"свой\",\\\n",
    " \"свою\",\"сделать\",\"сеаой\",\"себе\",\"себя\",\"сегодня\",\"седьмой\",\"сейчас\",\"семнадцатый\",\"семнадцать\",\"семь\",\"сидеть\",\"сила\",\"сих\",\"сказал\",\\\n",
    " \"сказала\",\"сказать\",\"сколько\",\"слишком\",\"слово\",\"случай\",\"смотреть\",\"сначала\",\"снова\",\"со\",\"собой\",\"собою\",\"советский\",\"совсем\",\\\n",
    " \"спасибо\",\"спросить\",\"сразу\",\"стал\",\"старый\",\"стать\",\"стол\",\"сторона\",\"стоять\",\"страна\",\"суть\",\"считать\",\"т\",\"та\",\"так\",\"такая\",\\\n",
    " \"также\",\"таки\",\"такие\",\"такое\",\"такой\",\"там\",\"твои\",\"твой\",\"твоя\",\"твоё\",\"те\",\"тебе\",\"тебя\",\"тем\",\"теми\",\"теперь\",\"тех\",\"то\",\"тобой\",\\\n",
    " \"тобою\",\"товарищ\",\"тогда\",\"того\",\"тоже\",\"только\",\"том\",\"тому\",\"тот\",\"тою\",\"третий\",\"три\",\"тринадцатый\",\"тринадцать\",\"ту\",\"туда\",\"тут\",\\\n",
    " \"ты\",\"тысяч\",\"у\",\"увидеть\",\"уж\",\"уже\",\"улица\",\"уметь\",\"утро\",\"хороший\",\"хорошо\",\"хотел бы\",\"хотеть\",\"хоть\",\"хотя\",\"хочешь\",\"час\",\\\n",
    " \"часто\",\"часть\",\"чаще\",\"чего\",\"человек\",\"чем\",\"чему\",\"через\",\"четвертый\",\"четыре\",\"четырнадцатый\",\"четырнадцать\",\"что\",\"чтоб\",\"чтобы\",\\\n",
    " \"чуть\",\"шестнадцатый\",\"шестнадцать\",\"шестой\",\"шесть\",\"эта\",\"эти\",\"этим\",\"этими\",\"этих\",\"это\",\"этого\",\"этой\",\"этом\",\"этому\",\"этот\",\\\n",
    " \"эту\",\"я\",\"являюсь\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('russian') + additional)\n",
    "\n",
    "# чтобы быстрее нормализовать тексты, создадим словарь всех словоформ\n",
    "# нормазуем каждую 1 раз и положим в словарь\n",
    "# затем пройдем по текстам и сопоставим каждой словоформе её нормальную форму\n",
    "\n",
    "def opt_normalize(texts, top=None):\n",
    "    uniq = Counter()\n",
    "    for text in texts:\n",
    "        uniq.update(text)\n",
    "    \n",
    "    norm_uniq = {word:morph.parse(word)[0].normal_form for word, _ in uniq.most_common(top)}\n",
    "    \n",
    "    norm_texts = []\n",
    "    for text in texts:\n",
    "        \n",
    "        norm_words = [norm_uniq.get(word) for word in text]\n",
    "        norm_words = [word for word in norm_words if word and word not in stops]\n",
    "        norm_texts.append(norm_words)\n",
    "        \n",
    "    return norm_texts\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    выбрасываем цифры (не нужны)\n",
    "    \"\"\"\n",
    "    words = re.findall(r\"[а-яА-ЯёЁ]+-*[а-яА-ЯёЁ]*\", text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = open('wiki_data.txt', encoding='utf-8').read().splitlines()[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = opt_normalize([tokenize(text.lower()) for text in texts], 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменён порог для получения интерпретируемых биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для нграммов\n",
    "ph = gensim.models.Phrases(texts, scoring='npmi', threshold=0.6)\n",
    "p = gensim.models.phrases.Phraser(ph)\n",
    "ngrammed_texts = p[texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA без TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictinary = gensim.corpora.Dictionary(ngrammed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictinary.filter_extremes(no_above=0.075, no_below=20)\n",
    "dictinary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(4170 unique tokens: ['активно', 'берег_река', 'близость', 'возраст', 'данные']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictinary.doc2bow(text) for text in ngrammed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_1 = gensim.models.LdaMulticore(corpus, 200, alpha=0.11, eta=0.66, id2word=dictinary, eval_every=0, passes=15)\n",
    "lda_2 = gensim.models.LdaMulticore(corpus, 150, alpha=0.33, eta=0.33, id2word=dictinary, eval_every=0, passes=20)\n",
    "lda_3 = gensim.models.LdaMulticore(corpus, 100, alpha=0.66, eta='auto', id2word=dictinary, eval_every=0, passes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #1 stats:\n",
      "Perplexity: -10.10597786498808\n",
      "\n",
      "\n",
      "Model #2 stats:\n",
      "Perplexity: -10.272879521020124\n",
      "\n",
      "\n",
      "Model #3 stats:\n",
      "Perplexity: -10.129609938905116\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, model in enumerate([lda_1, lda_2, lda_3]):\n",
    "    print(f\"Model #{index+1} stats:\")\n",
    "    print(f\"Perplexity: {model.log_perplexity(corpus[:1000])}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наилучшие результаты показывает модель №1, выводим три лучшие темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ТЕМА 1\n",
    "(194,\n",
    "  '0.009*\"миссия\" + 0.006*\"индеец\" + 0.002*\"поселение\" \n",
    "  + 0.002*\"северный_америка\" + 0.002*\"племя\" \n",
    "  + 0.002*\"орден\" + 0.002*\"французский\" \n",
    "  + 0.002*\"христианство\" + 0.001*\"деятельность\" + 0.001*\"квебек\"')\n",
    "\n",
    "ТЕМА 2\n",
    "(193,\n",
    "  '0.015*\"мозг\" + 0.011*\"поражение\" + 0.006*\"кровь\" \n",
    "  + 0.006*\"клетка\" + 0.006*\"ядро\" + 0.005*\"реакция\" \n",
    "  + 0.005*\"волокно\" + 0.005*\"больной\" + 0.004*\"верхний\" \n",
    "  + 0.004*\"возникать\"')\n",
    "\n",
    "ТЕМА 3\n",
    "(44,\n",
    "  '0.011*\"альбом\" + 0.009*\"нил\" + 0.008*\"выпустить\" \n",
    "  + 0.007*\"гитарист\" + 0.005*\"играть\" + 0.004*\"американский\" \n",
    "  + 0.004*\"покинуть\" + 0.003*\"гитара\" + 0.003*\"участник\" \n",
    "  + 0.003*\"дебютный_альбом\"')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_1.print_topics()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = gensim.models.CoherenceModel(model=lda_1, \n",
    "                                                  texts=ngrammed_texts, \n",
    "                                                   dictionary=dictinary, coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for topic_id, topic in lda_1.show_topics(num_topics=100, formatted=False):\n",
    "    topic = [word for word, _ in topic]\n",
    "    topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = gensim.models.CoherenceModel(topics=topics, \n",
    "                                                   texts=ngrammed_texts, \n",
    "                                                   dictionary=dictinary, coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47054040356373505"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добавление tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.models import ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus, id2word=dictinary)\n",
    "corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_4 = gensim.models.LdaModel(corpus, 100, alpha='auto', eta='auto', id2word=dictinary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_5 = gensim.models.LdaMulticore(corpus, 125, alpha='symmetric', eta='auto', id2word=dictinary, passes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_6 = gensim.models.LdaMulticore(corpus, 150, workers=2, alpha='asymmetric', eta='auto', id2word=dictinary, passes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model #4 stats:\n",
      "Perplexity: -20.69278501358015\n",
      "\n",
      "\n",
      "Model #5 stats:\n",
      "Perplexity: -49.46851254502912\n",
      "\n",
      "\n",
      "Model #6 stats:\n",
      "Perplexity: -38.29892329695855\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, model in enumerate([lda_4, lda_5, lda_6]):\n",
    "    print(f\"Model #{index+4} stats:\")\n",
    "    print(f\"Perplexity: {model.log_perplexity(corpus[:1000])}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Из моделей 4-6 лучший результат показывает модель №4 за счёт автоматического подбора параметров альфа и ета. В целом для tf-idf характерно уменьшение perplexity на данном корпусе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = gensim.models.CoherenceModel(model=lda_4, \n",
    "                                                  texts=ngrammed_texts, \n",
    "                                                   dictionary=dictinary, coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for topic_id, topic in lda_4.show_topics(num_topics=100, formatted=False):\n",
    "    topic = [word for word, _ in topic]\n",
    "    topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = gensim.models.CoherenceModel(topics=topics, \n",
    "                                                   texts=ngrammed_texts, \n",
    "                                                   dictionary=dictinary, coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117951170713938"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Когерентность топиков снизилась. Это отразилось и на их составе: внутри топиков попадаются слова-\"выбросы\", в том числе в наиболее когерентных. \n",
    "\n",
    "### Состав топиков почти не повторяет предыдущую модель, кроме \"спортивной\", \"военной\", \"музыкальной\" и \"административной\" (статьи про населённые пункты) тем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ТЕМА 1\n",
    "(13,\n",
    "  '0.059*\"музей\" + 0.053*\"художник\" + 0.031*\"выставка\" \n",
    "  + 0.028*\"документ\" + 0.023*\"картина\" + 0.023*\"париж\" \n",
    "  + 0.021*\"вариант\" + 0.019*\"сочинение\" + 0.017*\"лидер\" \n",
    "  + 0.016*\"журналист\"')\n",
    "\n",
    "ТЕМА 2\n",
    "(17,\n",
    "  '0.018*\"население\" + 0.014*\"деревня\" \n",
    "  + 0.013*\"административный_центр\" + 0.013*\"мужчина\" \n",
    "  + 0.012*\"норвегия\" + 0.011*\"н\" + 0.011*\"относиться\" \n",
    "  + 0.011*\"образовать\" + 0.010*\"площадь\" + 0.009*\"местный\"')\n",
    "\n",
    "ТЕМА 3\n",
    "(90,\n",
    "  '0.038*\"медаль\" + 0.025*\"ссср\" + 0.022*\"великий\" \n",
    "  + 0.019*\"партия\" + 0.018*\"международный\" + 0.016*\"совет\" \n",
    "  + 0.015*\"золотой\" + 0.013*\"президент\" + 0.013*\"соревнование\" \n",
    "  + 0.012*\"флаг\"')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn принимает на вход строки, поэтому склеим наши списки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stexts = [' '.join(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем матрицу слова-документы с помощью TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=1750, min_df=15, max_df=0.15, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(stexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разложим её."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components - главный параметр в NMF, это количество тем. \n",
    "# Если данных много, то увеличения этого параметра сильно увеличивает время обучения\n",
    "model_1 = NMF(l1_ratio=0, solver='mu', alpha=0.2, beta_loss='kullback-leibler', max_iter=250, n_components=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = NMF(l1_ratio=0.75, solver='cd', shuffle=True, beta_loss='frobenius', max_iter=200, n_components=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = NMF(l1_ratio=0.33, init='nndsvd', solver='mu', beta_loss='frobenius', max_iter=150, n_components=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_1, model_2, model_3]:\n",
    "    model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model number 1:\n",
      "Reconstruction error 1516.0672522532923\n",
      "\n",
      "\n",
      "Model number 2:\n",
      "Reconstruction error 1006.467619954277\n",
      "\n",
      "\n",
      "Model number 3:\n",
      "Reconstruction error 1033.1316375141125\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, model in enumerate([model_1, model_2, model_3]):\n",
    "    print(f'Model number {index+1}:')\n",
    "    print(f'Reconstruction error {model.reconstruction_err_}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лучшую статистику показывает модель №2. Ниже выведены 3 удачные темы.\n",
    "\n",
    "### Прежние темы (перечислены в tf-idf) остаются, к ним прибавляется несколько новых содержательных тем: \"морская\", \"экспедиционная\" (см. ниже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на матрицу темы-слова, отсортируем её по строкам и возьмем топ N слов, сопоставив индексы со словарём"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words = model_2.components_.argsort()[:,:-5:-1]\n",
    "\n",
    "for i in range(top_words.shape[0]):\n",
    "    words = [feat_names[j] for j in top_words[i]]\n",
    "    print(i, \"  \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Тема 1:\n",
    "экспедиция  лёд  северный  полюс\n",
    "\n",
    "Тема 2:\n",
    "флотилия  капитан  лейтенант  порт\n",
    "\n",
    "Тема 3:\n",
    "группа  участник  сингл  выйти\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_1, model_2, model_3, stexts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
