{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа, Игнатьев Д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм symspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    sents = sentenize(text)\n",
    "    return [normalize(sent.text) for sent in sents]\n",
    "\n",
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('data/sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "file = open('data/correct_sents.txt', encoding='utf8').read()\n",
    "true = file.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('data/wiki_data.txt', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### рекурсивная функция для удаления символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits(word, n_edits=1):\n",
    "    spellings = []\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:]               for L, R in splits if R]\n",
    "    spellings += deletes\n",
    "    if n_edits == 1:\n",
    "        return spellings\n",
    "    else:\n",
    "        for n in deletes:\n",
    "            spellings += edits(n, n_edits-1)\n",
    "        return spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_list = re.findall('\\w+', corpus.lower())\n",
    "probs = Counter(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фунцкия расчета вероятности слова\n",
    "N = sum(probs.values())\n",
    "def P(word, N=N): \n",
    "    \"Вычисляем вероятность слова\"\n",
    "    return probs[word] / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.879518450892764e-07"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(\"смешарики\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10177\n"
     ]
    }
   ],
   "source": [
    "vocab_list = re.findall('\\w+', file.lower())\n",
    "print(len(vocab_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация словаря вариантов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем словарь\n",
    "vocab = {n:n for n in vocab_list}\n",
    "\n",
    "for word in vocab_list:\n",
    "    options = edits(word, n_edits=2)\n",
    "    for n in options:\n",
    "        if n not in vocab:\n",
    "            vocab[n] = word\n",
    "        else:\n",
    "            # Проверка, что слово не было ранее добавлено через запятую\n",
    "            if re.compile(word).search(vocab[n]) == None:\n",
    "                vocab[n] += f\", {word}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции исправления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrections(word, vocab=vocab, n_edits=1):\n",
    "    corrected = []\n",
    "    options = edits(word, n_edits=n_edits)\n",
    "    if word in vocab and vocab[word] == word:\n",
    "        return [word]\n",
    "    for n in options:\n",
    "        if n in vocab:\n",
    "            corrected += vocab[n].split(\", \")\n",
    "        \n",
    "    if len(corrected) == 0:\n",
    "        return [word]\n",
    "    return list(set(corrected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['холод',\n",
       " 'колой',\n",
       " 'волос',\n",
       " 'мало',\n",
       " 'моего',\n",
       " 'много',\n",
       " 'масло',\n",
       " 'мол',\n",
       " 'моих',\n",
       " 'молока',\n",
       " 'голос',\n",
       " 'блоха',\n",
       " 'полно',\n",
       " 'мылом',\n",
       " 'можно',\n",
       " 'долго',\n",
       " 'плохо',\n",
       " 'около',\n",
       " 'молчу',\n",
       " 'обоих',\n",
       " 'могла']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrections('молох', n_edits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка работы алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
    "# разобьем предложение по пробелам и удалим пунктуация на границах слов\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
    "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0.586913086913087\n",
      "0.5587106676899463\n",
      "0.40886642930975076\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        # чтобы два раза не исправлять одно и тоже слово - закешируем его\n",
    "        # перед тем как считать исправление проверим нет ли его в кеше\n",
    "        predicted = cashed.get(pair[1], max(corrections(pair[1], n_edits=1), key=P))\n",
    "        cashed[pair[0]] = predicted\n",
    "        \n",
    "        \n",
    "        if predicted == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  predicted:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == predicted:\n",
    "                mistaken_fixed += 1\n",
    "        \n",
    "    if not i % 100:\n",
    "        print(i)\n",
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### При расстоянии редактирования 2 в словаре охват ошибок выше половины, но значительно число исправлений корректных слов.\n",
    "### При расстоянии редактирования 1 в словаре меньше неправильных исправлений (20%), но доля исправленных ошибок снижается до 30%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исправления с триграммной моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wiki = [['<start>', '<start>'] + sent + ['<end>'] for sent in preprocess(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter()\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "\n",
    "for sentence in corpus_wiki:\n",
    "    unigrams.update(sentence)\n",
    "    bigrams.update(ngrammer(sentence))\n",
    "    trigrams.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "# оцените качество также как и раньше\n",
    "def predict_mistaken(word, vocab):\n",
    "    if word in vocab:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    \n",
    "    word_pairs = [('<start>', '<start>')] + word_pairs\n",
    "    pred_sent = []\n",
    "    for j in range(1, len(word_pairs)):\n",
    "        \n",
    "        pred = None\n",
    "        \n",
    "        # проверяем, что слова нет в словаре, чтобы не исправлять все слова\n",
    "        if not predict_mistaken(word_pairs[j][1], probs):\n",
    "            pred = word_pairs[j][1]\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            # находим кандидатов для исправления\n",
    "            predicted = corrections(pair[1], n_edits=1)\n",
    "            #get_closest_hybrid_match(word_pairs[j][1], X, vec)\n",
    "        \n",
    "            # берем биграм для контекста\n",
    "            if j == 1:\n",
    "                preceding_bigram = '<start> <start>'\n",
    "            else:\n",
    "                preceding_bigram = word_pairs[j-2][0] + \" \" + word_pairs[j-1][0]\n",
    "#             prev_word = word_pairs[j-1][1]\n",
    "        \n",
    "            # если у нас нет в модели такого биграмма\n",
    "            if preceding_bigram not in bigrams:\n",
    "                pred = max(corrections(word_pairs[j][1], n_edits=1), key=P)\n",
    "        \n",
    "            else:\n",
    "                #\n",
    "                lm_predicted = []\n",
    "                for word in predicted:\n",
    "                    # поскольку нет возможности учитывать косинусную близость, будем умножать на вероятность самого слова\n",
    "                    m = P(word)\n",
    "                    trigram = preceding_bigram + \" \" + word\n",
    "                    # домножаем полученную метрику для слова на вероятность триграма\n",
    "                    # триграм - предыдущий биграм + текущее слово кандидат\n",
    "                    lm_predicted.append((word, (m)*((trigrams[trigram]/bigrams[preceding_bigram])))) \n",
    "                \n",
    "                \n",
    "                if lm_predicted:\n",
    "                    pred = sorted(lm_predicted, key=lambda x: -x[1])[0][0]\n",
    "        \n",
    "        if pred is None:\n",
    "            pred = word_pairs[j][1]\n",
    "\n",
    "        \n",
    "        if pred == word_pairs[j][0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((word_pairs[j][0], word_pairs[j][1], pred))\n",
    "        total += 1\n",
    "            \n",
    "        if word_pairs[j][0] == word_pairs[j][1]:\n",
    "            total_correct += 1\n",
    "            if word_pairs[j][0] !=  pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if word_pairs[j][0] == pred:\n",
    "                mistaken_fixed += 1\n",
    "    \n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8524475524475524\n",
      "0.12125863392171911\n",
      "0.03813024003675204\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Комментарий: добавление триграммной модели сильно минимизировало процент ненужных исправлений. При этом часть нужных исправлений также отпала, поскольку вероятность соответствующих биграмм оказалась небольшой"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
